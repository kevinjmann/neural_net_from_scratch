{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impl of neural network in python following the same structure as the c++ code using numpy for efficient calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "x, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "x = (x/255).astype('float32')\n",
    "x = x.to_numpy()\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.15, random_state=42)\n",
    "y_train_oh = to_categorical(y_train)\n",
    "y_val_oh = to_categorical(y_val)\n",
    "y_train = y_train.astype('int')\n",
    "y_val = y_val.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "class ActivationFunctionId(Enum):\n",
    "    RELU = 1\n",
    "    SIGMOID = 2\n",
    "    SOFTMAX = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activate(inputs, fxnId):\n",
    "    output = None\n",
    "    if fxnId == ActivationFunctionId.SIGMOID:\n",
    "        output = 1 / (1 + np.exp(-inputs))\n",
    "    elif fxnId == ActivationFunctionId.SOFTMAX:\n",
    "        e_x = np.exp(inputs - np.max(inputs))\n",
    "        output = e_x / np.sum(e_x, axis = 0)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    weights = None\n",
    "    biases = None  # biases.shape has to be (numOutputFeatures, 1)\n",
    "    deltaWeights = None\n",
    "    deltaBiases = None\n",
    "    previousOutput = None\n",
    "    activationFunction = None\n",
    "    nodeLocalGradients = None\n",
    "\n",
    "    def __init__(self, inputFeatures, outputFeatures, activationFunctionId):\n",
    "        self.weights = np.random.randn(outputFeatures, inputFeatures) * np.sqrt(1 / outputFeatures)\n",
    "        self.biases = np.zeros((outputFeatures, 1))\n",
    "        self.activationFunction = activationFunctionId\n",
    "\n",
    "    def forward(self, input):\n",
    "        unactivated = np.matmul(self.weights, input) #+ self.biases\n",
    "        self.previousOutput = activate(unactivated, self.activationFunction)\n",
    "        return self.previousOutput\n",
    "\n",
    "    def backward(self, leftLayer, sampleIdx):\n",
    "        curSample = leftLayer.previousOutput[:, sampleIdx].copy()  # 1d np array\n",
    "        self.deltaWeights += np.outer(self.nodeLocalGradients, curSample)\n",
    "        return np.matmul(np.transpose(self.weights), self.nodeLocalGradients) * curSample * (1 - curSample)  # column vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    layers = []\n",
    "    learningRate = 1e-1\n",
    "\n",
    "    def forward(self, input):\n",
    "        assert len(self.layers) > 0\n",
    "        self.layers[0].forward(input)\n",
    "        for i in range(1, len(self.layers)):\n",
    "            self.layers[i].forward(self.layers[i-1].previousOutput)\n",
    "        return self.layers[-1].previousOutput\n",
    "\n",
    "    def backpropagation(self, inputBatch, labels):\n",
    "        # initialize the weight and bias update values for a batch\n",
    "        for layer in self.layers:\n",
    "            layer.deltaWeights = np.zeros(layer.weights.shape)\n",
    "        # calculate\n",
    "        tmp = self.layers[-1].previousOutput.copy()\n",
    "        tmp[labels, np.arange(tmp.shape[1])] -= 1\n",
    "        batchOutputDeltas = 2 * tmp / tmp.shape[0]\n",
    "        for i in range(inputBatch.shape[1]):\n",
    "            self.layers[-1].nodeLocalGradients = batchOutputDeltas[:, i].copy() # 1d numpy array\n",
    "            for l in range(len(self.layers) -1, 0, -1):\n",
    "                self.layers[l-1].nodeLocalGradients = self.layers[l].backward(self.layers[l-1], i)\n",
    "            inputLayerDeltaWeights = np.outer(self.layers[0].nodeLocalGradients, inputBatch[:, i])\n",
    "            self.layers[0].deltaWeights += inputLayerDeltaWeights\n",
    "\n",
    "        for layer in self.layers:\n",
    "            layer.weights -= self.learningRate *layer.deltaWeights / inputBatch.shape[1]\n",
    "\n",
    "\n",
    "    def get_accuracy(self, batch, labels):\n",
    "        output = self.forward(batch)\n",
    "        return np.mean(np.argmax(output, axis=0) == labels)\n",
    "\n",
    "\n",
    "    def get_loss(self, batch, labels, eps=1e-7):\n",
    "        output = self.forward(batch)\n",
    "        clipped = np.clip(output, eps, 1 - eps)[labels, np.arange(len(labels))]\n",
    "        neg_log = -np.log(clipped)\n",
    "        return np.sum(neg_log) / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    network = None\n",
    "    dataX = None\n",
    "    dataY = None\n",
    "    batchesX = None\n",
    "    batchesY = None\n",
    "    def __init__(self, network, dataX, dataY):\n",
    "        self.network = network\n",
    "        self.dataX, self.dataY = (dataX, dataY)\n",
    "\n",
    "    def train(self, batchSize, numEpochs):\n",
    "        batchesX = np.split(self.dataX, len(self.dataX) / batchSize)\n",
    "        batchesY = np.split(self.dataY, len(self.dataY) / batchSize)\n",
    "        for epoch in range(numEpochs):\n",
    "            for batchIdx in range(len(batchesX)):\n",
    "                batch = np.transpose(batchesX[batchIdx])\n",
    "                labels = batchesY[batchIdx]\n",
    "                network.forward(batch)\n",
    "                network.backpropagation(batch, labels)\n",
    "            val_acc = network.get_accuracy(np.transpose(x_val), y_val)\n",
    "            val_loss = network.get_loss(np.transpose(x_val), y_val)\n",
    "            print(f\"end epoch {epoch} with validation accuracy {val_acc} and loss {val_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end epoch 0 with validation accuracy 0.8923809523809524 and loss 0.3972930986634044\n",
      "end epoch 1 with validation accuracy 0.9108571428571428 and loss 0.31636767138263844\n",
      "end epoch 2 with validation accuracy 0.9204761904761904 and loss 0.2805386971509729\n"
     ]
    }
   ],
   "source": [
    "network = Network()\n",
    "network.layers = [\n",
    "    Layer(784, 128, ActivationFunctionId.SIGMOID),\n",
    "    Layer(128, 64, ActivationFunctionId.SIGMOID),\n",
    "    Layer(64, 10, ActivationFunctionId.SOFTMAX)\n",
    "    ]\n",
    "\n",
    "t = Trainer(network, x_train, y_train)\n",
    "t.train(10, 3)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fe39e3bc97668c0da3f3a9011a8720ed40d06770c114da623ff1628d4614fcc4"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('3.7.11')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
